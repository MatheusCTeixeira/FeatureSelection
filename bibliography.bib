@article{Khalifa2016,
abstract = {MicroRNAs (miRNAs) are short nucleotide sequences that form a typical hairpin structure which is recognized by a complex enzyme machinery. It ultimately leads to the incorporation of 18-24 nt long mature miRNAs into RISC where they act as recognition keys to aid in regulation of target mRNAs. It is involved to determine miRNAs experimentally and, therefore, machine learning is used to complement such endeavors. The success of machine learning mostly depends on proper input data and appropriate features for parameterization of the data. Although, in general, two-class classification (TCC) is used in the field; because negative examples are hard to come by, one-class classification (OCC) has been tried for pre-miRNA detection. Since both positive and negative examples are currently somewhat limited, feature selection can prove to be vital for furthering the field of pre-miRNA detection. In this study, we compare the performance of OCC and TCC using eight feature selection methods and seven different plant species providing positive pre-miRNA examples. Feature selection was very successful for OCC where the best feature selection method achieved an average accuracy of 95.6{\%}, thereby being {\~{}}29{\%} better than the worst method which achieved 66.9{\%} accuracy. While the performance is comparable to TCC, which performs up to 3{\%} better than OCC, TCC is much less affected by feature selection and its largest performance gap is {\~{}}13{\%} which only occurs for two of the feature selection methodologies. We conclude that feature selection is crucially important for OCC and that it can perform on par with TCC given the proper set of features.},
author = {Khalifa, Waleed and Yousef, Malik and {Sa{\c{c}}ar Demirci}, M{\"{u}}ÅŸerref Duygu and Allmer, Jens},
doi = {10.7717/peerj.2135},
file = {:home/matheusc/Documents/Computa{\c{c}}{\~{a}}o Natural/Papers/The impact of feature selection on one and two-class classification performance for plant microRNAs.pdf:pdf},
issn = {21678359},
journal = {PeerJ},
keywords = {Feature selection,Machine learning,MicroRNA,One-class classification,Plant,Two-class classification},
number = {6},
pages = {1--13},
title = {{The impact of feature selection on one and two-class classification performance for plant microRNAs}},
volume = {2016},
year = {2016}
}
@article{Zhu2018,
abstract = {This paper proposes a new unsupervised spectral feature selection method to preserve both the local and global structure of the features as well as the samples. Specifically, our method uses the self-expressiveness of the features to represent each feature by other features for preserving the local structure of features, and a low-rank constraint on the weight matrix to preserve the global structure among samples as well as features. Our method also proposes to learn the graph matrix measuring the similarity of samples for preserving the local structure among samples. Furthermore, we propose a new optimization algorithm to the resulting objective function, which iteratively updates the graph matrix and the intrinsic space so that collaboratively improving each of them. Experimental analysis on 12 benchmark datasets showed that the proposed method outperformed the state-of-the-art feature selection methods in terms of classification performance.},
author = {Zhu, Xiaofeng and Zhang, Shichao and Hu, Rongyao and Zhu, Yonghua and Song, Jingkuan},
doi = {10.1109/TKDE.2017.2763618},
file = {:home/matheusc/Documents/Computa{\c{c}}{\~{a}}o Natural/Papers/Local and Global Structure Preservation for.pdf:pdf},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Feature selection,dimensionality reduction,graph matrix,subspace learning},
number = {3},
pages = {517--529},
title = {{Local and Global Structure Preservation for Robust Unsupervised Spectral Feature Selection}},
volume = {30},
year = {2018}
}
@article{Zheng2020,
abstract = {Previous feature selection methods equivalently consider the samples to select important features. However, the samples are often diverse. For example, the outliers should have small or even zero weights while the important samples should have large weights. In this paper, we add a self-paced regularization in the sparse feature selection model to reduce the impact of outliers for conducting feature selection. Specifically, the proposed method automatically selects a sample subset which includes the most important samples to build an initial feature selection model, whose generalization ability is then improved by involving other important samples until a robust and generalized feature selection model has been established or all the samples have been used. Experimental results on eight real datasets show that the proposed method outperforms the comparison methods.},
author = {Zheng, Wei and Zhu, Xiaofeng and Wen, Guoqiu and Zhu, Yonghua and Yu, Hao and Gan, Jiangzhang},
doi = {10.1016/j.patrec.2018.06.029},
file = {:home/matheusc/Documents/Computa{\c{c}}{\~{a}}o Natural/Papers/Unsupervised feature selection by self-paced learning regularization.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Feature selection,Robust statistic,Self-paced learning},
pages = {4--11},
publisher = {Elsevier B.V.},
title = {{Unsupervised feature selection by self-paced learning regularization}},
url = {https://doi.org/10.1016/j.patrec.2018.06.029},
volume = {132},
year = {2020}
}
@article{Mafarja2018,
abstract = {Classification accuracy highly dependents on the nature of the features in a dataset which may contain irrelevant or redundant data. The main aim of feature selection is to eliminate these types of features to enhance the classification accuracy. The wrapper feature selection model works on the feature set to reduce the number of features and improve the classification accuracy simultaneously. In this work, a new wrapper feature selection approach is proposed based on Whale Optimization Algorithm (WOA). WOA is a newly proposed algorithm that has not been systematically applied to feature selection problems yet. Two binary variants of the WOA algorithm are proposed to search the optimal feature subsets for classification purposes. In the first one, we aim to study the influence of using the Tournament and Roulette Wheel selection mechanisms instead of using a random operator in the searching process. In the second approach, crossover and mutation operators are used to enhance the exploitation of the WOA algorithm. The proposed methods are tested on standard benchmark datasets and then compared to three algorithms such as Particle Swarm Optimization (PSO), Genetic Algorithm (GA), the Ant Lion Optimizer (ALO), and five standard filter feature selection methods. The paper also considers an extensive study of the parameter setting for the proposed technique. The results show the efficiency of the proposed approaches in searching for the optimal feature subsets.},
author = {Mafarja, Majdi and Mirjalili, Seyedali},
doi = {10.1016/j.asoc.2017.11.006},
file = {:home/matheusc/Documents/Computa{\c{c}}{\~{a}}o Natural/Papers/Whale Optimization Approaches for Wrapper Feature Selection.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Classification,Crossover,Evolutionary operators,Feature selection,Mutation,Optimization,Selection,WOA,Whale optimization algorithm},
pages = {441--453},
publisher = {Elsevier B.V.},
title = {{Whale optimization approaches for wrapper feature selection}},
url = {http://dx.doi.org/10.1016/j.asoc.2017.11.006},
volume = {62},
year = {2018}
}
@article{Fidalgo2001,
abstract = {Feature subset selection is a central issue in a vast diversity of problems including classification, function approximation, machine learning and adaptive control. On a wide variety of applications, especially when using real data, input features may be not independent and output variable depends on the relationship among inputs rather than on input values themselves. Feature selection methods that assume independence of attributes will fail on these cases. On the other side, most of alternative approaches are quasi-exhaustive, requiring large CPU processing time. In this paper, an alternative methodology based on sensitivity analysis of trained artificial neural networks (ANN) is analyzed. Results so far attained on illustrative toy examples and on real data support the validity of the developed approach.},
author = {Fidalgo, J. N.},
file = {:home/matheusc/Documents/Computa{\c{c}}{\~{a}}o Natural/Papers/Feature Subset Selection Based on ANN.pdf:pdf},
isbn = {9608052262},
journal = {Advances in Neural Networks and Applications},
keywords = {Correlation,Feature selection,Neural networks,Sensitivity},
number = {1},
pages = {206--211},
title = {{Feature subset selection based on ANN sensitivity analysis - A practical study}},
year = {2001}
}
@article{Perera2019,
abstract = {We present a novel deep-learning-based approach for one-class transfer learning in which labeled data from an unrelated task is used for feature learning in one-class classification. The proposed method operates on top of a convolutional neural network (CNN) of choice and produces descriptive features while maintaining a low intra-class variance in the feature space for the given class. For this purpose two loss functions, compactness loss and descriptiveness loss, are proposed along with a parallel CNN architecture. A template matching-based framework is introduced to facilitate the testing process. Extensive experiments on publicly available anomaly detection, novelty detection, and mobile active authentication datasets show that the proposed deep one-class (DOC) classification method achieves significant improvements over the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1801.05365},
author = {Perera, Pramuditha and Patel, Vishal M.},
doi = {10.1109/TIP.2019.2917862},
eprint = {1801.05365},
file = {:home/matheusc/Documents/Computa{\c{c}}{\~{a}}o Natural/Papers/Learning Deep Features for One-Class Classification.pdf:pdf},
issn = {19410042},
journal = {IEEE Transactions on Image Processing},
keywords = {One-class classification,anomaly detection,deep learning,novelty detection},
number = {11},
pages = {5450--5463},
pmid = {31144635},
title = {{Learning Deep Features for One-Class Classification}},
volume = {28},
year = {2019}
}
@article{Lorena2015,
abstract = {In one-class classification problems all training examples belong to a single class. The absence of counter-examples represents a challenge to traditional Machine Learning and pre-processing techniques. This is the case of various feature selection techniques for labeled data. The selection of the most relevant features from a dataset usually benefits the performance obtained by classification algorithms. Despite the relevance of this issue, few techniques have been proposed for feature selection in one-class classification problems. Moreover, most of the existent techniques are wrapper approaches, which have to rely on a specific classification algorithm for feature selection, or aggregation techniques. This paper proposes a new filter feature selection approach for one-class classification. First, five feature selection measures from different paradigms are here employed or adapted to the one-class scenario. Next, the feature rankings produced by these measures are combined using different aggregation strategies. The proposed approach was able to reduce the size of the feature sets while maintaining or even improving the predictive performance obtained by the one-class classifier.},
author = {Lorena, Luiz H.N. and Carvalho, Andr{\'{e}} C.P.L.F. and Lorena, Ana C.},
doi = {10.1007/s10846-014-0101-2},
file = {:home/matheusc/Documents/Computa{\c{c}}{\~{a}}o Natural/Papers/Filter Feature Selection for One-Class Classification.pdf:pdf},
issn = {15730409},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Filter feature selection,One-class classification,Rank aggregation},
number = {Icmc},
pages = {227--243},
title = {{Filter Feature Selection for One-Class Classification}},
volume = {80},
year = {2015}
}
@article{Miao2016,
abstract = {Feature selection, as a dimensionality reduction technique, aims to choosing a small subset of the relevant features from the original features by removing irrelevant, redundant or noisy features. Feature selection usually can lead to better learning performance, i.e., higher learning accuracy, lower computational cost, and better model interpretability. Recently, researchers from computer vision, text mining and so on have proposed a variety of feature selection algorithms and in terms of theory and experiment, show the effectiveness of their works. This paper is aimed at reviewing the state of the art on these techniques. Furthermore, a thorough experiment is conducted to check if the use of feature selection can improve the performance of learning, considering some of the approaches mentioned in the literature. The experimental results show that unsupervised feature selection algorithms benefits machine learning tasks improving the performance of clustering.},
archivePrefix = {arXiv},
arxivId = {1510.02892},
author = {Miao, Jianyu and Niu, Lingfeng},
doi = {10.1016/j.procs.2016.07.111},
eprint = {1510.02892},
file = {:home/matheusc/Documents/Computa{\c{c}}{\~{a}}o Natural/Papers/A Survey on Feature Selection.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {clustering,feature selection,machine learning,unsupervised},
number = {Itqm},
pages = {919--926},
publisher = {Elsevier Masson SAS},
title = {{A Survey on Feature Selection}},
url = {http://dx.doi.org/10.1016/j.procs.2016.07.111},
volume = {91},
year = {2016}
}
@article{Chandrashekar2014,
abstract = {Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction performance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in literature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Chandrashekar, Girish and Sahin, Ferat},
doi = {10.1016/j.compeleceng.2013.11.024},
file = {:home/matheusc/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chandrashekar, Sahin - 2014 - A survey on feature selection methods.pdf:pdf},
issn = {00457906},
journal = {Computers and Electrical Engineering},
number = {1},
pages = {16--28},
publisher = {Elsevier Ltd},
title = {{A survey on feature selection methods}},
url = {http://dx.doi.org/10.1016/j.compeleceng.2013.11.024},
volume = {40},
year = {2014}
}
@article{Sheikhpour2017,
abstract = {Feature selection is a significant task in data mining and machine learning applications which eliminates irrelevant and redundant features and improves learning performance. In many real-world applications, collecting labeled data is difficult, while abundant unlabeled data are easily accessible. This motivates researchers to develop semi-supervised feature selection methods which use both labeled and unlabeled data to evaluate feature relevance. However, till-to-date, there is no comprehensive survey covering the semi-supervised feature selection methods. In this paper, semi-supervised feature selection methods are fully investigated and two taxonomies of these methods are presented based on two different perspectives which represent the hierarchical structure of semi-supervised feature selection methods. The first perspective is based on the basic taxonomy of feature selection methods and the second one is based on the taxonomy of semi-supervised learning methods. This survey can be helpful for a researcher to obtain a deep background in semi-supervised feature selection methods and choose a proper semi-supervised feature selection method based on the hierarchical structure of them.},
author = {Sheikhpour, Razieh and Sarram, Mehdi Agha and Gharaghani, Sajjad and Chahooki, Mohammad Ali Zare},
doi = {10.1016/j.patcog.2016.11.003},
file = {:home/matheusc/Documents/Computa{\c{c}}{\~{a}}o Natural/Papers/A Survey on semi-supervised feature selection methods.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Feature selection,Semi-supervised learning,Survey},
pages = {141--158},
publisher = {Elsevier},
title = {{A Survey on semi-supervised feature selection methods}},
url = {http://dx.doi.org/10.1016/j.patcog.2016.11.003},
volume = {64},
year = {2017}
}
@article{Centner1996,
abstract = {A new method for the elimination of uninformative variables in multivariate data sets is proposed. To achieve this, artificial (noise) variables are added and a closed form of the PLS or PCR model is obtained for the data set containing the experimental and the artificial variables. The experimental variables that do not have more importance than the artificial variables, as judged from a criterion based on the b coefficients, are eliminated. The performance of the method is evaluated on simulated data. Practical aspects are discussed on experimentally obtained near-IR data sets. It is concluded that the elimination of uninformative variables can improve predictive ability.},
author = {Centner, V{\'{i}}t{\'{e}}zslav and Massart, D{\'{e}}sir{\'{e}} Luc and {De Noord}, Onno E. and {De Jong}, Sijmen and Vandeginste, Bernard M. and Sterna, C{\'{e}}cile},
doi = {10.1021/ac960321m},
file = {:home/matheusc/Documents/Computa{\c{c}}{\~{a}}o Natural/Papers/Elimination of Uninformative Variables for Multivariate Calibration.pdf:pdf},
issn = {00032700},
journal = {Analytical Chemistry},
number = {21},
pages = {3851--3858},
title = {{Elimination of Uninformative Variables for Multivariate Calibration}},
volume = {68},
year = {1996}
}
@article{Leardi1998,
abstract = {Genetic algorithms (GA) are very useful in solving complex problems of optimization. The selection of the best subset of variables is surely one of them. In this paper, a new approach is proposed, and the positive and negative aspects of the application of GA in selecting variables for a partial least squares (PLS) model are taken into account. Finally, the analysis of the results obtained on several real data sets allows to find a rationale for a sensible application, showing that, if correctly applied, this technique almost always produces very good results.},
author = {Leardi, Riccardo and {Lupi{\'{a}}{\~{n}}ez Gonz{\'{a}}lez}, Amparo},
doi = {10.1016/S0169-7439(98)00051-3},
file = {:home/matheusc/Documents/Computa{\c{c}}{\~{a}}o Natural/Papers/Genetic algorithms applied to feature selection in PLS regression$\backslash$: how and when to use them.pdf:pdf},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {Feature selection,Genetic algorithms,PLS regression},
number = {2},
pages = {195--207},
title = {{Genetic algorithms applied to feature selection in PLS regression: How and when to use them}},
volume = {41},
year = {1998}
}
@article{Rodriguez-Ruiz2020,
abstract = {Twitter is a popular online social network with hundreds of millions of users, where n important part of the accounts in this social network are not humans. Approximately 48 million Twitter accounts are managed by automated programs called bots, which represents up to 15{\%} of all accounts. Some bots have good purposes, such as automatically posting information about news and academic papers, and even to provide help during emergencies. Nevertheless, Twitter bots have also been used for malicious purposes, such as distributing malware or influencing the perception of the public about a topic. There are existing mechanisms that allow detecting bots on Twitter automatically; however, these mechanisms rely on examples of existing bots to discern them from legitimate accounts. As the bot landscape changes, with the bot creators using more sophisticated methods to avoid detection, new mechanisms for discerning between legitimate and bot accounts are needed. In this paper, we propose to use one-class classification to enhance Twitter bot detection, as this allows detecting novel bot accounts, and requires only from examples of legitimate accounts. Our experiment results show that our proposal can consistently detect different types of bots with a performance above 0.89 measured using AUC, without requiring previous information about them.},
author = {Rodr{\'{i}}guez-Ruiz, Jorge and Mata-S{\'{a}}nchez, Javier Israel and Monroy, Ra{\'{u}}l and Loyola-Gonz{\'{a}}lez, Octavio and L{\'{o}}pez-Cuevas, Armando},
doi = {10.1016/j.cose.2020.101715},
file = {:home/matheusc/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rodr{\'{i}}guez-Ruiz et al. - 2020 - A one-class classification approach for bot detection on Twitter.pdf:pdf},
issn = {01674048},
journal = {Computers and Security},
keywords = {Anomaly detection,One-class classifiers,Social networks,Supervised classification,Twitter bot detection},
month = {apr},
publisher = {Elsevier Ltd},
title = {{A one-class classification approach for bot detection on Twitter}},
volume = {91},
year = {2020}
}
@article{Bessi2016,
abstract = {Social media have been extensively praised for increasing democratic discussion on social issues related to policy and politics. However, what happens when this powerful communication tools are exploited to manipulate online discussion, to change the public perception of political entities, or even to try affecting the outcome of political elections? In this study we investigated how the presence of social media bots, algorithmically driven entities that on the surface appear as legitimate users, affect political discussion around the 2016 U.S. Presidential election. By leveraging state-of-the-art social bot detection algorithms, we uncovered a large fraction of user population that may not be human, accounting for a significant portion of generated content (about one-fifth of the entire conversation). We inferred political partisanships from hashtag adoption, for both humans and bots, and studied spatio-temporal communication, political support dynamics, and influence mechanisms by discovering the level of network embeddedness of the bots. Our findings suggest that the presence of social media bots can indeed negatively affect democratic political discussion rather than improving it, which in turn can potentially alter public opinion and endanger the integrity of the Presidential election.},
author = {Bessi, Alessandro and Ferrara, Emilio},
doi = {http://dx.doi.org/10.5210/fm.v21i11.7090},
file = {:home/matheusc/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bessi, Ferrara - 2016 - Social bots distort the 2016 US Presidential election online discussion.pdf:pdf},
journal = {First Monday},
title = {{Social bots distort the 2016 US Presidential election online discussion}},
volume = {21},
year = {2016}
}
@book{PiotrJUSZCZAK2006,
abstract = {The thesis treats classification problems which are undersampled or where there exist an$\backslash$nunbalance between classes in the sampling. The thesis is divided into three parts. The first two$\backslash$nparts treat the problem of one-class classification. In the one-class classification problem, it is$\backslash$nassumed that only examples of one of the classes, the target class, are available. The fact that$\backslash$nno (or almost no) examples of other classes are available makes the one-class classification an$\backslash$nexample of an extremely unbalance problem. Therefore, such problem can not be described$\backslash$naccurately by existing multi-class classifiers. However, a need to solve such classification$\backslash$nrises from many theoretical and practical problems, e.g. the concept learning, machine fault$\backslash$ndetection and face recognition.$\backslash$nIn the third part of the thesis, we treat classification problems which are undersampled$\backslash$nbut not necessary unbalanced. In such problems, additional examples or additional knowledge$\backslash$nabout data available during training significantly improves classification performance. We$\backslash$ninvestigate two types of enhancement of a small training set with additional knowledge from$\backslash$na large unlabelled data set: active learning and semi-supervised sampling.},
author = {{Piotr JUSZCZAK}},
booktitle = {Community nurse},
file = {:home/matheusc/Documents/Computa{\c{c}}{\~{a}}o Natural/Papers/Learning to recognise. A study on one-class classification and active learning..pdf:pdf},
isbn = {9789090206844},
number = {8},
pages = {18--9},
title = {{Learning to recognise. A study on one-class classification and active learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21320521},
volume = {4},
year = {2006}
}
@book{Liu1998,
address = {Boston, MA},
author = {Liu, Huan and Motoda, Hiroshi},
doi = {10.1007/978-1-4615-5689-3},
file = {:home/matheusc/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Motoda - 1998 - Feature Selection for Knowledge Discovery and Data Mining.pdf:pdf},
isbn = {978-1-4613-7604-0},
publisher = {Springer US},
title = {{Feature Selection for Knowledge Discovery and Data Mining}},
url = {http://link.springer.com/10.1007/978-1-4615-5689-3},
year = {1998}
}
@article{Khan2014,
abstract = {One-class classification (OCC) algorithms aim to build classification models when the negative class is either absent, poorly sampled or not well defined. This unique situation constrains the learning of efficient classifiers by defining class boundary just with the knowledge of positive class. The OCC problem has been considered and applied under many research themes, such as outlier/novelty detection and concept learning. In this paper, we present a unified view of the general problem of OCC by presenting a taxonomy of study for OCC problems, which is based on the availability of training data, algorithms used and the application domains applied. We further delve into each of the categories of the proposed taxonomy and present a comprehensive literature review of the OCC algorithms, techniques and methodologies with a focus on their significance, limitations and applications. We conclude our paper by discussing some open research problems in the field of OCC and present our vision for future research.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.0049v1},
author = {Khan, Shehroz S. and Madden, Michael G.},
doi = {10.1017/S026988891300043X},
eprint = {arXiv:1312.0049v1},
file = {:home/matheusc/Documents/Computa{\c{c}}{\~{a}}o Natural/Papers/One-Class Classification$\backslash$: Taxonomy of Study and Review of Techniques.pdf:pdf},
isbn = {0000000000000},
issn = {0269-8889},
journal = {The Knowledge Engineering Review},
month = {jun},
number = {3},
pages = {345--374},
title = {{One-class classification: taxonomy of study and review of techniques}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Technical+Forum+Group+on+Agents+in+Bioinformatics+1{\#}0 https://www.cambridge.org/core/product/identifier/S026988891300043X/type/journal{\_}article},
volume = {29},
year = {2014}
}
